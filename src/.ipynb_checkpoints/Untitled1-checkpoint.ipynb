{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import lexApr as lex\n",
    "import CSR\n",
    "import subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'neutral' 'surprise' 'happiness' 'anger']\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "Categories (6, int64): [0, 1, 2, 3, 4, 5]\n",
      "POS Tagging and cleaning...\n",
      "Sentence tokenization...\n",
      "LEXICON-BASED METHOD WITH LEXICON: NRC-Emotion-Lexicon-v0.92-English.csv\n",
      "Opening the lexicon...\n",
      "Tokenizing...\n",
      "Creating the tuple...\n",
      "Finding the sentiment...\n"
     ]
    }
   ],
   "source": [
    "#Open the training dataset\n",
    "training_set = \"../Data/train_data_red.csv\"\n",
    "train_data = pd.read_csv(training_set, engine='python')\n",
    "    \n",
    "#dropna drops missing values(not available)\n",
    "train_data = train_data.dropna(axis=0)\n",
    "print (train_data.sentiment.unique())\n",
    "\n",
    "#GET THE RAW FEATURES\n",
    "\n",
    "X = train_data.content\n",
    "#change the value of sentiment from string to int\n",
    "y = pd.Categorical(pd.factorize(train_data.sentiment)[0])\n",
    "print (y.unique())\n",
    "\n",
    "#Clean the data and replace each seperator with a single fullstop\n",
    "#also we use POS tagging for emoticons, urls , @-mentions and hashtags\n",
    "print (\"POS Tagging and cleaning...\")\n",
    "\n",
    "X = [re.sub(r'[^\\x00-\\x7f]',r' ',s) for s in X]             #remove non-ascii characters\n",
    "X = [re.sub(r'https?:\\/\\/[^ ]*',r'URL',s) for s in X]       #replace urls\n",
    "#replace the negative or positive emoticons with tags\n",
    "pos_regex = '[:;]-?[)Dp]+|<3'\n",
    "neg_regex = ':-?\\'?[(/Oo]+'\n",
    "X = [re.sub(pos_regex, ' posE ',s) for s in X]\n",
    "X = [re.sub(neg_regex, ' negE ',s) for s in X]\n",
    "\n",
    "X = [re.sub(r'[.,!;?:]+',r'. ',s) for s in X]             #replace seperators for tokenization\n",
    "X = [re.sub(r'#[^ ]*',r'HASHTAG', s) for s in X]          #replace hashtags\n",
    "X = [re.sub(r'@[^ ]*',r'AT_MENTION', s) for s in X]       #replace @-mentions\n",
    "X = [re.sub(\"[^A-Za-z_.' ]+\",r' ', s) for s in X]\n",
    "\n",
    "#Tokenize each microblog text into sentences\n",
    "print (\"Sentence tokenization...\")\n",
    "X = [word_tokenize(s) for s in X]\n",
    "\n",
    "#find is sentences contain conjuction words\n",
    "#if they do split them and save the position of the conjuction word\n",
    "\n",
    "#open conjuctions.txt and save each word to list\n",
    "word_list = [line.rstrip('\\n') for line in open(\"../Data/conjunctions.txt\")]\n",
    "\n",
    "XX = []\n",
    "for tweet in X:\n",
    "    tmp = []\n",
    "    s = ''\n",
    "    for word in tweet:\n",
    "        if word in word_list:\n",
    "            if s != '':\n",
    "                tmp.append(s)\n",
    "            tmp.append(word)\n",
    "            s = word\n",
    "        elif word == '.':\n",
    "            if s != '':\n",
    "                tmp.append(s)\n",
    "            s = ''\n",
    "        else:\n",
    "            if s == '':\n",
    "                s += word\n",
    "            else:\n",
    "                s += \" \" + word\n",
    "    if s != '':\n",
    "        tmp.append(s)\n",
    "    XX.append(tmp)\n",
    "\n",
    "#-----------------PART 1-----------------#\n",
    "#lexicon-based method\n",
    "emo = lex.lex(XX)\n",
    "\n",
    "ruleitems = []\n",
    "Xvals = []\n",
    "for i,v in enumerate(XX):\n",
    "    tmp = []\n",
    "    for j,sent in enumerate(v):\n",
    "        if sent in word_list:\n",
    "            tmp.append(sent)\n",
    "        else:\n",
    "            if emo[i][j][0] == emo[i][j][1]:\n",
    "                tmp.append(str(emo[i][j][0]))\n",
    "            else:\n",
    "                tmp.append((str(emo[i][j][0]), str(emo[i][j][1])))\n",
    "    Xvals.append(tmp)\n",
    "    ruleitems.append([tmp,y[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CSR-apriori on data with minsup 0.01% and minconf 0.5%...\n",
      "[('5', '0'), 'either', '0', ('4', '0'), 'for', ('5', '1'), ('5', '4'), ('1', '0'), 'so', ('4', '0'), 'if', 'but', 'so', ('5', '1'), '0', 'or', ('1', '5'), 'cause', ('4', '1'), ('4', '3'), 'for', ('1', '5'), 'when', 'if', 'for', 'so', ('5', '1'), ('1', '3'), 'for', '0', ('4', '3'), ('3', '0'), 'til', ('4', '0'), 'til', ('5', '0'), ('1', '0'), 'because', '0', 'but', ('4', '1'), ('5', '4'), '0', (('1', '0'), '0'), (('1', '0'), 'but'), (('1', '0'), 'either'), (('1', '5'), ('4', '0')), (('1', '5'), '0'), (('1', '5'), 'so'), (('4', '0'), ('5', '0')), (('4', '0'), '0'), (('4', '0'), 'but'), (('4', '0'), 'til'), (('4', '0'), '0'), (('4', '0'), 'so'), (('4', '0'), '0'), (('4', '1'), '0'), (('4', '3'), '0'), (('4', '3'), 'but'), (('4', '3'), 'for'), (('4', '3'), '0'), (('5', '0'), '0'), (('5', '0'), '0'), (('5', '0'), 'for'), (('5', '0'), 'or'), (('5', '0'), 'when'), (('5', '1'), '0'), (('5', '1'), 'so'), (('5', '1'), '0'), (('5', '1'), 'but'), (('5', '4'), '0'), (('5', '4'), 'for'), ('0', 'for'), ('0', 'because'), ('0', 'but'), ('0', 'cause'), ('0', 'either'), ('0', 'for'), ('0', 'if'), ('0', 'so'), ('0', 'til'), ('0', 'but'), ('0', 'for'), ('0', 'if'), ('0', 'or'), ('0', 'so'), ('0', 'til'), ('0', 'when'), ('0', 'for'), ('0', 'so'), ('because', 'either'), ('or', 'when'), (('4', '0'), ('5', '0'), '0'), (('4', '3'), '0', 'but'), (('5', '0'), '0', 'for')]\n"
     ]
    }
   ],
   "source": [
    "#-----------------PART 2-----------------#\n",
    "#Minning Class Sequential Rules\n",
    "csrs = CSR.CSR_apriori(ruleitems[:100], 0.0001, 0.005)\n",
    "print(csrs)\n",
    "Xtrain = []\n",
    "for x in Xvals:\n",
    "    Xtrain.append([1 if subset.subset(i,x) else 0 for i in csrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "take_nd() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f9e2eac04b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#split the training data in order to get a cross validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mycv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mycv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantinos/.local/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantinos/.local/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((a,))\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantinos/.local/lib/python2.7/site-packages/sklearn/utils/__init__.pyc\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: take_nd() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    " #SVM on all data\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "\n",
    "#split the training data in order to get a cross validation set\n",
    "xtr, xcv, ytr, ycv = train_test_split(Xtrain, y, random_state=42, test_size=0.2)\n",
    "print(len(xtr),len(ytr),len(xcv),len(ycv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the svm\n",
    "#using linear kernel\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(xtr, ytr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
