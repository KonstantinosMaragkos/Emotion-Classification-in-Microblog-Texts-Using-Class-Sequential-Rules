{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'neutral' 'surprise' 'happiness' 'anger']\n",
      "POS Tagging and cleaning...\n",
      "Sentence tokenization...\n",
      "LEXICON-BASED METHOD WITH LEXICON: NRC-Emotion-Lexicon-v0.92-English.csv\n",
      "Opening the lexicon...\n",
      "Tokenizing...\n",
      "Creating the tuple...\n",
      "Finding the sentiment...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import lexApr as lex\n",
    "import CSR, subset\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#Open the training dataset\n",
    "training_set = \"../Data/train_data_red.csv\"\n",
    "train_data = pd.read_csv(training_set, engine='python')\n",
    "\n",
    "#dropna drops missing values(not available)\n",
    "train_data = train_data.dropna(axis=0)\n",
    "print (train_data.sentiment.unique())\n",
    "\n",
    "#GET THE RAW FEATURES\n",
    "\n",
    "X = train_data.content\n",
    "#change the value of sentiment from string to int\n",
    "#y = pd.Categorical(pd.factorize(train_data.sentiment)[0])\n",
    "y = []\n",
    "for s in train_data.sentiment:\n",
    "    switch = {\n",
    "    'empty': 0,\n",
    "    'sadness':  1,\n",
    "    'neutral':  2,\n",
    "    'surprise':  3,\n",
    "    'happiness':  4,\n",
    "    'anger': 5\n",
    "    }\n",
    "    y.append(switch.get(s))\n",
    "\n",
    "#Clean the data and replace each seperator with a single fullstop\n",
    "#also we use POS tagging for emoticons, urls , @-mentions and hashtags\n",
    "print (\"POS Tagging and cleaning...\")\n",
    "\n",
    "X = [re.sub(r'[^\\x00-\\x7f]',r' ',s) for s in X]             #remove non-ascii characters\n",
    "X = [re.sub(r'https?:\\/\\/[^ ]*',r'URL',s) for s in X]       #replace urls\n",
    "#replace the negative or positive emoticons with tags\n",
    "pos_regex = '[:;]-?[)Dp]+|<3'\n",
    "neg_regex = ':-?\\'?[(/Oo]+'\n",
    "X = [re.sub(pos_regex, ' posE ',s) for s in X]\n",
    "X = [re.sub(neg_regex, ' negE ',s) for s in X]\n",
    "\n",
    "X = [re.sub(r'[.,!;?:]+',r'. ',s) for s in X]             #replace seperators for tokenization\n",
    "X = [re.sub(r'#[^ ]*',r'HASHTAG', s) for s in X]          #replace hashtags\n",
    "X = [re.sub(r'@[^ ]*',r'AT_MENTION', s) for s in X]       #replace @-mentions\n",
    "X = [re.sub(\"[^A-Za-z_.' ]+\",r' ', s) for s in X]\n",
    "\n",
    "#Tokenize each microblog text into sentences\n",
    "print (\"Sentence tokenization...\")\n",
    "X = [word_tokenize(s) for s in X]\n",
    "\n",
    "#find is sentences contain conjuction words\n",
    "#if they do split them and save the position of the conjuction word\n",
    "\n",
    "#open conjuctions.txt and save each word to list\n",
    "word_list = [line.rstrip('\\n') for line in open(\"../Data/conjunctions.txt\")]\n",
    "\n",
    "XX = []\n",
    "for tweet in X:\n",
    "    tmp = []\n",
    "    s = ''\n",
    "    for word in tweet:\n",
    "        if word in word_list:\n",
    "            if s != '':\n",
    "                tmp.append(s)\n",
    "            tmp.append(word)\n",
    "            s = word\n",
    "        elif word == '.':\n",
    "            if s != '':\n",
    "                tmp.append(s)\n",
    "            s = ''\n",
    "        else:\n",
    "            if s == '':\n",
    "                s += word\n",
    "            else:\n",
    "                s += \" \" + word\n",
    "    if s != '':\n",
    "        tmp.append(s)\n",
    "    XX.append(tmp)\n",
    "\n",
    "#-----------------PART 1-----------------#\n",
    "#lexicon-based method\n",
    "emo = lex.lex(XX)\n",
    "\n",
    "ruleitems = []\n",
    "Xvals = []\n",
    "for i,v in enumerate(XX):\n",
    "    tmp = []\n",
    "    for j,sent in enumerate(v):\n",
    "        if sent in word_list:\n",
    "            tmp.append(sent)\n",
    "        else:\n",
    "            if emo[i][j][0] == emo[i][j][1]:\n",
    "                tmp.append(str(emo[i][j][0]))\n",
    "            else:\n",
    "                tmp.append((str(emo[i][j][0]), str(emo[i][j][1])))\n",
    "    Xvals.append(tmp)\n",
    "    ruleitems.append([tmp,y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CSR-apriori on data with minsup 0.01% and minconf 0.5%...\n",
      "[('4', '3'), 'so', 'for', ('4', '0'), '0', ('4', '0'), ('1', '0'), ('4', '0'), ('4', '3'), '0', ('5', '0'), ('4', '3'), '0', ('5', '1'), 'so', 'but', '0', 'for', 'but', ('5', '0'), 'for', 'but', ('4', '0'), ('5', '1'), 'for', ('4', '3'), 'so', '0', ('1', '0'), (('1', '0'), '0'), (('4', '0'), '0'), (('4', '0'), '0'), (('4', '0'), '0'), (('4', '3'), '0'), (('4', '3'), '0'), (('4', '3'), '0'), (('5', '1'), '0'), ('0', 'but'), ('0', 'for'), ('0', 'so'), ('0', 'but'), ('0', 'for'), ('0', 'so'), ('0', 'for'), ('0', 'so')]\n"
     ]
    }
   ],
   "source": [
    "#-----------------PART 2-----------------#\n",
    "#Minning Class Sequential Rules\n",
    "csrs = CSR.CSR_apriori(ruleitems, 0.0001, 0.005)\n",
    "print(csrs)\n",
    "Xtrain = []\n",
    "for x in Xvals:\n",
    "    Xtrain.append([1 if subset.subset(i,x) else 0 for i in csrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantinos/.local/lib/python2.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "0.3745839636913767\n"
     ]
    }
   ],
   "source": [
    "#SVM on all data\n",
    "\n",
    "#split the training data in order to get a cross validation set\n",
    "xtr, xcv, ytr, ycv = train_test_split(Xtrain, y, random_state=42, test_size=0.2)\n",
    "\n",
    "#fit the svm using a linear kernel\n",
    "print(\"Training the classifier...\")\n",
    "svc = svm.SVC(kernel='poly', degree=4)\n",
    "svc = svc.fit(xtr[:3000], ytr[:3000])\n",
    "\n",
    "print(\"Predicting...\")\n",
    "prediction = svc.predict(xcv)\n",
    "score = f1_score(ycv, prediction, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
