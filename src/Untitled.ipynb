{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "training_set = \"../Data/train_data.csv\"\n",
    "train_data = pd.read_csv(training_set, engine='python')\n",
    "\n",
    "#dropna drops missing values(not available)\n",
    "train_data = train_data.dropna(axis=0)\n",
    "#print train_data.sentiment.unique()\n",
    "\n",
    "X = train_data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tweets and POS Tagging...\n",
      "Tokenizing...\n"
     ]
    }
   ],
   "source": [
    "#open the lexicon\n",
    "lexicon = \"../Data/NRC-Emotion-Lexicon-v0.92-English.csv\"\n",
    "lex_data = pd.read_csv(lexicon, engine='python')\n",
    "emos = ['Positive','Negative','Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust']\n",
    "\n",
    "#prepare the tweets\n",
    "print \"Cleaning tweets and POS Tagging...\"\n",
    "document = [re.sub(r'[^\\x00-\\x7f]',r' ',s) for s in X]          #remove non-ascii characters\n",
    "#POS tag hashtags, @-mentions, remove whitespace and unnecessary symbols(;:.'\")\n",
    "document = [re.sub(r'https?:\\/\\/[^ ]*',r'[URL]',s) for s in document]\n",
    "document = [re.sub(r'#[^ ]*',r'[HASHTAG]', s) for s in document]\n",
    "document = [re.sub(r'@[^ ]*',r'[AT_MENTION]', s) for s in document]\n",
    "document = [re.sub(\"[^A-Za-z_' ]+\",r' ', s) for s in document]\n",
    "\n",
    "#tokenize them and for each word create a tuple\n",
    "#with the word and the number of times it exists in the tweet\n",
    "print \"Tokenizing...\"\n",
    "tokens = []\n",
    "for tweet in document:\n",
    "    tokens.append([w.lower() for w in word_tokenize(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords...\n",
      "25%\n",
      "50%\n",
      "75%\n",
      "100%\n",
      "Lemmatizing...\n",
      "[['at_mention', 'know', 'listenin', 'bad', 'habit', 'earlier', u'start', 'freakin', 'part'], ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'], ['funeral', 'ceremony', 'gloomy', 'friday'], [u'want', 'hang', u'friend', 'soon'], ['at_mention', 'want', 'trade', 'someone', 'houston', u'ticket', 'one'], [u'ping', 'at_mention', \"n't\", 'go', 'prom', 'bc', 'bf', \"n't\", 'like', u'friend'], ['sleep', 'im', u'think', 'old', 'friend', 'want', \"'s\", 'married', 'damn', 'amp', u'want', 'scandalous'], ['hmmm', 'url'], ['at_mention', 'charlene', 'love', 'miss'], ['at_mention', \"'m\", 'sorry', 'least', \"'s\", 'friday']]\n",
      "Creating the tuple...\n"
     ]
    }
   ],
   "source": [
    "print \"Removing Stopwords...\"\n",
    "#remove stopwords\n",
    "filtered = []\n",
    "for i,lst in enumerate(tokens):\n",
    "    if i == len(tokens)/4:\n",
    "        print \"25%\"\n",
    "    if i == len(tokens)/2:\n",
    "        print \"50%\"\n",
    "    if i == (3*len(tokens))/4:\n",
    "        print \"75%\"\n",
    "    filtered.append([w for w in lst if not w in stopwords.words('english')])\n",
    "print \"100%\"\n",
    "\n",
    "print \"Lemmatizing...\"\n",
    "#Lemmatize with POS Tags\n",
    "#it may take some minutes !!\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lems = []\n",
    "for lst in filtered:\n",
    "    lems.append([ lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lst ])\n",
    "\n",
    "print lems[:10]\n",
    "\n",
    "print \"Creating the tuple...\"\n",
    "tuples = []\n",
    "for lst in lems:\n",
    "    count = Counter(lst)\n",
    "    tuples.append(count.most_common(len(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hate', 4), ('cancer', 1)]\n"
     ]
    }
   ],
   "source": [
    "print tuples[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each word in the tweet\n",
    "results = []\n",
    "for lst in tuples[:100]:\n",
    "    vals = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for word, count in lst:\n",
    "        tmp = count * lex_data[emos].where(lex_data['English'] == word).dropna(axis=0).values\n",
    "        if len(tmp) != 0:\n",
    "            vals += tmp;\n",
    "    results.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 5. 5. 0. 5. 5. 0. 5. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print results[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
