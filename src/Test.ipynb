{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import lexApr as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging and cleaning...\n",
      "Sentence tokenization...\n"
     ]
    }
   ],
   "source": [
    "#Open the training dataset\n",
    "training_set = \"../Data/train_data.csv\"\n",
    "train_data = pd.read_csv(training_set, engine='python')\n",
    "\n",
    "#dropna drops missing values(not available)\n",
    "train_data = train_data.dropna(axis=0)\n",
    "#print train_data.sentiment.unique()\n",
    "\n",
    "#GET THE RAW FEATURES\n",
    "\n",
    "X = train_data.content\n",
    "#change the value of sentiment from string to int\n",
    "y = pd.Categorical(pd.factorize(train_data.sentiment)[0])\n",
    "\n",
    "\n",
    "#Clean the data and replace each seperator with a single fullstop\n",
    "#also we use POS tagging for emoticons, urls , @-mentions and hashtags\n",
    "print \"POS Tagging and cleaning...\"\n",
    "\n",
    "X = [re.sub(r'[^\\x00-\\x7f]',r' ',s) for s in X]             #remove non-ascii characters\n",
    "X = [re.sub(r'https?:\\/\\/[^ ]*',r'URL',s) for s in X]     #replace urls\n",
    "#replace the negative or positive emoticons with tags\n",
    "pos_regex = '[:;]-?[)Dp]+|<3'\n",
    "neg_regex = ':-?\\'?[(/Oo]+'\n",
    "X = [re.sub(pos_regex, ' posE ',s) for s in X]\n",
    "X = [re.sub(neg_regex, ' negE ',s) for s in X]\n",
    "\n",
    "X = [re.sub(r'[.,!;?:]+',r'. ',s) for s in X]                #replace seperators\n",
    "X = [re.sub(r'#[^ ]*',r'HASHTAG', s) for s in X]          #replace hashtags\n",
    "X = [re.sub(r'@[^ ]*',r'AT_MENTION', s) for s in X]       #replace @-mentions\n",
    "X = [re.sub(\"[^A-Za-z_.' ]+\",r' ', s) for s in X]\n",
    "\n",
    "#Tokenize each microblog text into sentences\n",
    "print \"Sentence tokenization...\"\n",
    "X =  [sent_tokenize(s) for s in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AT_MENTION i know  i was listenin to bad habit earlier and i started freakin at his part'], ['Layin n bed with a headache  ughhhh.', 'waitin on your call.'], ['Funeral ceremony.', 'gloomy friday.'], ['wants to hang out with friends SOON.'], ['AT_MENTION We want to trade with someone who has Houston tickets.', 'but no one will.'], [\"Re pinging AT_MENTION  why didn't you go to prom.\", \"BC my bf didn't like my friends\"], ['I should be sleep.', 'but im not.', 'thinking about an old friend who I want.', \"but he's married now.\", 'damn.', 'amp.', 'he wants me  .', 'scandalous.'], ['Hmmm.', 'URL is down'], ['AT_MENTION Charlene my love.', 'I miss you'], [\"AT_MENTION I'm sorry  at least it's Friday.\"], ['cant fall asleep'], ['Choked on her retainers'], ['Ugh.', 'I have to beat this stupid song to get to the next  rude.'], ['AT_MENTION if u watch the hills in london u will realise what tourture it is because were weeks and weeks late  i just watch itonlinelol'], ['Got the news'], ['The storm is here and the electricity is gone'], ['AT_MENTION agreed'], [\"So sleepy again and it's not even that late.\", 'I fail once again.'], ['AT_MENTION lady gaga tweeted about not being impressed by her video leaking just so you know'], ['How are YOU convinced that I have always wanted you.', 'What signals did I give off.', 'damn I think I just lost another friend']]\n"
     ]
    }
   ],
   "source": [
    "print X[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [re.sub(\"[^A-Za-z_' ]+\",r' ', s1) for s in X for s1 in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AT_MENTION i know  i was listenin to bad habit earlier and i started freakin at his part', 'Layin n bed with a headache  ughhhh ', 'waitin on your call ', 'Funeral ceremony ', 'gloomy friday ', 'wants to hang out with friends SOON ', 'AT_MENTION We want to trade with someone who has Houston tickets ', 'but no one will ', \"Re pinging AT_MENTION  why didn't you go to prom \", \"BC my bf didn't like my friends\", 'I should be sleep ', 'but im not ', 'thinking about an old friend who I want ', \"but he's married now \", 'damn ', 'amp ', 'he wants me   ', 'scandalous ', 'Hmmm ', 'URL is down']\n"
     ]
    }
   ],
   "source": [
    "print X[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
